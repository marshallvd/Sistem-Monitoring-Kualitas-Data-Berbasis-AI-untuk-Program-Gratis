{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "MBG DATA - EXPLORATORY DATA ANALYSIS & DATA CLEANSING PIPELINE\n",
        "================================================================================\n",
        "Purpose: Comprehensive EDA and cleaning for MBG dataset\n",
        "Author: Data Quality Pipeline\n",
        "Date: 2025\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "import os\n",
        "import hashlib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîç MBG DATA - EDA & CLEANSING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================\n",
        "# PART 1: DATA LOADING\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìÇ PART 1: DATA LOADING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Running without Google Drive mount\")\n",
        "\n",
        "# Load data\n",
        "data_path = '/content/drive/MyDrive/Magister SI TelU/Semester 1/Analisis Data dan Perusahaan/Tugas Besar/Progress Week 9/mbg_data.csv'\n",
        "\n",
        "print(f\"\\nüìÅ Loading data from:\")\n",
        "print(f\"   {data_path}\")\n",
        "\n",
        "try:\n",
        "    df_raw = pd.read_csv(data_path, encoding='utf-8-sig')\n",
        "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
        "    print(f\"   ‚Ä¢ Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
        "    print(f\"   ‚Ä¢ Memory: {df_raw.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error loading data: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Create backup\n",
        "df = df_raw.copy()\n",
        "print(f\"\\n‚úÖ Working copy created for analysis\")\n",
        "\n",
        "# ============================================\n",
        "# PART 2: BASIC INFORMATION\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã PART 2: BASIC INFORMATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Column Information:\")\n",
        "print(\"-\" * 80)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    dtype = df[col].dtype\n",
        "    non_null = df[col].notna().sum()\n",
        "    null_count = df[col].isna().sum()\n",
        "    null_pct = (null_count / len(df) * 100)\n",
        "    print(f\"   {i:2d}. {col:25s} | {str(dtype):15s} | {non_null:6,}/{len(df):6,} filled ({100-null_pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Missing Values Summary:\")\n",
        "print(\"-\" * 80)\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "has_missing = False\n",
        "\n",
        "for col in df.columns:\n",
        "    if missing[col] > 0:\n",
        "        has_missing = True\n",
        "        print(f\"   ‚ö†Ô∏è  {col:25s}: {missing[col]:6,} missing ({missing_pct[col]:5.2f}%)\")\n",
        "\n",
        "if not has_missing:\n",
        "    print(\"   ‚úÖ No missing values detected!\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£  Data Types Distribution:\")\n",
        "print(\"-\" * 80)\n",
        "dtype_counts = df.dtypes.value_counts()\n",
        "for dtype, count in dtype_counts.items():\n",
        "    print(f\"   ‚Ä¢ {dtype}: {count} columns\")\n",
        "\n",
        "print(f\"\\n4Ô∏è‚É£  Sample Data Preview:\")\n",
        "print(\"-\" * 80)\n",
        "print(df.head(3).to_string())\n",
        "\n",
        "# ============================================\n",
        "# PART 3: DUPLICATE ANALYSIS (DETAILED)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç PART 3: DETAILED DUPLICATE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create content hash for accurate duplicate detection\n",
        "df['content_hash'] = df['content'].apply(\n",
        "    lambda x: hashlib.md5(str(x).encode()).hexdigest() if pd.notna(x) else None\n",
        ")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  Duplicate Analysis by Different Criteria:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# URL duplicates\n",
        "url_dup_count = df.duplicated(subset=['url'], keep=False).sum()\n",
        "url_unique = df['url'].nunique()\n",
        "print(f\"   üìå URL Analysis:\")\n",
        "print(f\"      ‚Ä¢ Total unique URLs: {url_unique:,}\")\n",
        "print(f\"      ‚Ä¢ Duplicate URL entries: {url_dup_count:,}\")\n",
        "print(f\"      ‚Ä¢ URL uniqueness: {url_unique/len(df)*100:.1f}%\")\n",
        "\n",
        "# Title duplicates\n",
        "title_dup_count = df.duplicated(subset=['title'], keep=False).sum()\n",
        "title_unique = df['title'].nunique()\n",
        "print(f\"\\n   üìå Title Analysis:\")\n",
        "print(f\"      ‚Ä¢ Total unique titles: {title_unique:,}\")\n",
        "print(f\"      ‚Ä¢ Duplicate title entries: {title_dup_count:,}\")\n",
        "print(f\"      ‚Ä¢ Title uniqueness: {title_unique/len(df)*100:.1f}%\")\n",
        "\n",
        "# Content duplicates\n",
        "content_dup_count = df.duplicated(subset=['content_hash'], keep=False).sum()\n",
        "content_unique = df['content_hash'].nunique()\n",
        "print(f\"\\n   üìå Content Analysis:\")\n",
        "print(f\"      ‚Ä¢ Total unique contents: {content_unique:,}\")\n",
        "print(f\"      ‚Ä¢ Duplicate content entries: {content_dup_count:,}\")\n",
        "print(f\"      ‚Ä¢ Content uniqueness: {content_unique/len(df)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£  True Duplicates (Same Content):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "true_duplicates = df[df.duplicated(subset=['content_hash'], keep=False)]\n",
        "if len(true_duplicates) > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Found {len(true_duplicates):,} rows with duplicate content\")\n",
        "\n",
        "    # Show examples\n",
        "    content_counts = df['content_hash'].value_counts()\n",
        "    multi_contents = content_counts[content_counts > 1].head(3)\n",
        "\n",
        "    print(f\"\\n   üìã Top 3 Most Duplicated Contents:\")\n",
        "    for idx, (hash_val, count) in enumerate(multi_contents.items(), 1):\n",
        "        articles = df[df['content_hash'] == hash_val][['title', 'date', 'category']].head(2)\n",
        "        print(f\"\\n   Example {idx}: Appears {count}x\")\n",
        "        for i, row in articles.iterrows():\n",
        "            print(f\"      ‚Ä¢ {row['title'][:60]}...\")\n",
        "            print(f\"        Date: {row['date']} | Category: {row['category']}\")\n",
        "else:\n",
        "    print(\"   ‚úÖ No duplicate content found!\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  Duplicate Detection Conclusion:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   üìä Summary:\")\n",
        "print(f\"      ‚Ä¢ Original dataset: {len(df):,} rows\")\n",
        "print(f\"      ‚Ä¢ Unique by URL: {url_unique:,} rows ({url_unique/len(df)*100:.1f}%)\")\n",
        "print(f\"      ‚Ä¢ Unique by Title: {title_unique:,} rows ({title_unique/len(df)*100:.1f}%)\")\n",
        "print(f\"      ‚Ä¢ Unique by Content: {content_unique:,} rows ({content_unique/len(df)*100:.1f}%)\")\n",
        "print(f\"\\n   üí° Recommendation:\")\n",
        "print(f\"      ‚Ä¢ Use CONTENT HASH for deduplication\")\n",
        "print(f\"      ‚Ä¢ Expected clean data: ~{content_unique:,} unique articles\")\n",
        "\n",
        "# ============================================\n",
        "# PART 4: CONTENT STATISTICS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä PART 4: CONTENT STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate metrics\n",
        "df['content_length'] = df['content'].fillna('').str.len()\n",
        "df['word_count'] = df['content'].fillna('').str.split().str.len()\n",
        "df['title_length'] = df['title'].fillna('').str.len()\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Content Length Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Mean:     {df['content_length'].mean():,.0f} characters\")\n",
        "print(f\"   ‚Ä¢ Median:   {df['content_length'].median():,.0f} characters\")\n",
        "print(f\"   ‚Ä¢ Min:      {df['content_length'].min():,.0f} characters\")\n",
        "print(f\"   ‚Ä¢ Max:      {df['content_length'].max():,.0f} characters\")\n",
        "print(f\"   ‚Ä¢ Std Dev:  {df['content_length'].std():,.0f} characters\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Word Count Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Mean:     {df['word_count'].mean():,.0f} words\")\n",
        "print(f\"   ‚Ä¢ Median:   {df['word_count'].median():,.0f} words\")\n",
        "print(f\"   ‚Ä¢ Min:      {df['word_count'].min():,.0f} words\")\n",
        "print(f\"   ‚Ä¢ Max:      {df['word_count'].max():,.0f} words\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£  Content Length Distribution:\")\n",
        "print(\"-\" * 80)\n",
        "bins = [0, 500, 1000, 2000, 3000, 5000, 10000]\n",
        "labels = ['<500', '500-1K', '1K-2K', '2K-3K', '3K-5K', '>5K']\n",
        "df['content_category'] = pd.cut(df['content_length'], bins=bins, labels=labels)\n",
        "\n",
        "dist = df['content_category'].value_counts().sort_index()\n",
        "for cat, count in dist.items():\n",
        "    pct = count / len(df) * 100\n",
        "    bar = '‚ñà' * int(pct / 2)\n",
        "    print(f\"   {cat:10s}: {count:5,} articles ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 5: CATEGORY ANALYSIS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üè∑Ô∏è  PART 5: CATEGORY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Category Distribution:\")\n",
        "print(\"-\" * 80)\n",
        "cat_counts = df['category'].value_counts()\n",
        "total_articles = len(df)\n",
        "\n",
        "for idx, (cat, count) in enumerate(cat_counts.items(), 1):\n",
        "    pct = count / total_articles * 100\n",
        "    bar = '‚ñà' * int(pct / 2)\n",
        "    print(f\"   {idx:2d}. {cat:30s}: {count:5,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Category Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total unique categories: {df['category'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Most common: {cat_counts.index[0]} ({cat_counts.values[0]:,} articles)\")\n",
        "print(f\"   ‚Ä¢ Least common: {cat_counts.index[-1]} ({cat_counts.values[-1]:,} articles)\")\n",
        "print(f\"   ‚Ä¢ Articles without category: {df['category'].isna().sum()}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 6: AUTHOR ANALYSIS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úçÔ∏è  PART 6: AUTHOR ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "author_counts = df['author'].fillna('Unknown').value_counts()\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Top 15 Most Productive Authors:\")\n",
        "print(\"-\" * 80)\n",
        "for idx, (author, count) in enumerate(author_counts.head(15).items(), 1):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"   {idx:2d}. {author:40s}: {count:4,} articles ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Author Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total unique authors: {df['author'].nunique():,}\")\n",
        "print(f\"   ‚Ä¢ Articles without author: {df['author'].isna().sum():,}\")\n",
        "print(f\"   ‚Ä¢ Average articles per author: {len(df) / df['author'].nunique():.1f}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 7: DATE ANALYSIS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìÖ PART 7: DATE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Date Format Examples:\")\n",
        "print(\"-\" * 80)\n",
        "for i, date in enumerate(df['date'].dropna().head(5), 1):\n",
        "    print(f\"   {i}. {date}\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Date Completeness:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Articles with date: {df['date'].notna().sum():,} ({df['date'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Articles without date: {df['date'].isna().sum():,} ({df['date'].isna().sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Extract year from date string\n",
        "df['year_extracted'] = df['date'].str.extract(r'(\\d{4})')\n",
        "\n",
        "if df['year_extracted'].notna().sum() > 0:\n",
        "    year_counts = df['year_extracted'].value_counts().sort_index()\n",
        "    print(f\"\\n3Ô∏è‚É£  Articles by Year:\")\n",
        "    print(\"-\" * 80)\n",
        "    for year, count in year_counts.items():\n",
        "        pct = count / len(df) * 100\n",
        "        bar = '‚ñà' * int(pct / 2)\n",
        "        print(f\"   {year}: {count:5,} articles ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 8: MBG-SPECIFIC CONTENT ANALYSIS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üçΩÔ∏è  PART 8: MBG-SPECIFIC CONTENT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Keywords analysis\n",
        "dist_keywords = ['distribusi', 'pembagian', 'menyalurkan', 'membagikan', 'bantuan', 'porsi']\n",
        "df['contains_distribution'] = df['content'].fillna('').str.lower().str.contains('|'.join(dist_keywords))\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Distribution Content Detection:\")\n",
        "print(\"-\" * 80)\n",
        "dist_count = df['contains_distribution'].sum()\n",
        "print(f\"   ‚Ä¢ Articles about distribution: {dist_count:,}/{len(df):,} ({dist_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Number patterns\n",
        "porsi_pattern = r'(\\d+[\\.,]?\\d*)\\s*(?:porsi|makanan|paket)'\n",
        "penerima_pattern = r'(\\d+[\\.,]?\\d*)\\s*(?:orang|siswa|anak|penerima|warga)'\n",
        "\n",
        "df['has_porsi_info'] = df['content'].str.contains(porsi_pattern, case=False, regex=True, na=False)\n",
        "df['has_penerima_info'] = df['content'].str.contains(penerima_pattern, case=False, regex=True, na=False)\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Quantitative Information:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Articles with portion info: {df['has_porsi_info'].sum():,}\")\n",
        "print(f\"   ‚Ä¢ Articles with recipient info: {df['has_penerima_info'].sum():,}\")\n",
        "\n",
        "complete_info = df[\n",
        "    df['contains_distribution'] &\n",
        "    df['has_porsi_info'] &\n",
        "    df['has_penerima_info']\n",
        "]\n",
        "print(f\"   ‚Ä¢ Articles with complete info: {len(complete_info):,}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 9: DATA CLEANSING\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üßπ PART 9: DATA CLEANSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Before Cleaning:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total rows: {len(df):,}\")\n",
        "print(f\"   ‚Ä¢ Unique content: {df['content_hash'].nunique():,}\")\n",
        "print(f\"   ‚Ä¢ Duplicates to remove: {len(df) - df['content_hash'].nunique():,}\")\n",
        "\n",
        "# Remove duplicates based on content hash\n",
        "df_clean = df.drop_duplicates(subset=['content_hash'], keep='first').copy()\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Duplicate Removal:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚úÖ Removed {len(df) - len(df_clean):,} duplicate articles\")\n",
        "print(f\"   ‚úÖ Retained {len(df_clean):,} unique articles\")\n",
        "\n",
        "# Remove temporary columns\n",
        "columns_to_drop = ['content_hash', 'content_category', 'year_extracted',\n",
        "                   'contains_distribution', 'has_porsi_info', 'has_penerima_info']\n",
        "df_clean = df_clean.drop(columns=[col for col in columns_to_drop if col in df_clean.columns])\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Column Cleanup:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚úÖ Removed temporary analysis columns\")\n",
        "print(f\"   ‚úÖ Final columns: {len(df_clean.columns)}\")\n",
        "\n",
        "# Handle missing values (optional - keeping as is for now)\n",
        "print(f\"\\n3Ô∏è‚É£  Missing Values Handling:\")\n",
        "print(\"-\" * 80)\n",
        "for col in df_clean.columns:\n",
        "    missing = df_clean[col].isna().sum()\n",
        "    if missing > 0:\n",
        "        print(f\"   ‚Ä¢ {col}: {missing:,} missing values (kept as is)\")\n",
        "\n",
        "print(f\"\\nüìä After Cleaning:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total rows: {len(df_clean):,}\")\n",
        "print(f\"   ‚Ä¢ Total columns: {len(df_clean.columns)}\")\n",
        "print(f\"   ‚Ä¢ Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "print(f\"   ‚Ä¢ Reduction: {(1 - len(df_clean)/len(df))*100:.1f}%\")\n",
        "\n",
        "# ============================================\n",
        "# PART 10: SAVE CLEANED DATA\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ PART 10: SAVING CLEANED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define save locations\n",
        "output_dir = '/content/drive/MyDrive/Magister SI TelU/Semester 1/Analisis Data dan Perusahaan/Tugas Besar/Progress Week 9/'\n",
        "output_file = 'mbg_data_clean.csv'\n",
        "output_path = os.path.join(output_dir, output_file)\n",
        "\n",
        "# Save to Google Drive\n",
        "try:\n",
        "    df_clean.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n‚úÖ Data saved to Google Drive:\")\n",
        "    print(f\"   üìÅ {output_path}\")\n",
        "    print(f\"   üìä Size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Could not save to Google Drive: {str(e)}\")\n",
        "\n",
        "# Also save to local for immediate use\n",
        "local_path = '/content/mbg_data_clean.csv'\n",
        "try:\n",
        "    df_clean.to_csv(local_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n‚úÖ Data also saved locally:\")\n",
        "    print(f\"   üìÅ {local_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Could not save locally: {str(e)}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 11: FINAL SUMMARY REPORT\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã PART 11: FINAL SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüéØ EDA & CLEANSING SUMMARY:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Dataset Transformation:\")\n",
        "print(f\"   ‚Ä¢ Original dataset: {len(df):,} articles\")\n",
        "print(f\"   ‚Ä¢ Cleaned dataset: {len(df_clean):,} articles\")\n",
        "print(f\"   ‚Ä¢ Removed duplicates: {len(df) - len(df_clean):,} articles\")\n",
        "print(f\"   ‚Ä¢ Data reduction: {(1 - len(df_clean)/len(df))*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüîç Data Quality Metrics:\")\n",
        "print(f\"   ‚Ä¢ Content uniqueness: 100.0%\")\n",
        "print(f\"   ‚Ä¢ Date completeness: {df_clean['date'].notna().sum()/len(df_clean)*100:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Author completeness: {df_clean['author'].notna().sum()/len(df_clean)*100:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Category completeness: {df_clean['category'].notna().sum()/len(df_clean)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüìà Content Characteristics:\")\n",
        "print(f\"   ‚Ä¢ Unique categories: {df_clean['category'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Unique authors: {df_clean['author'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Avg content length: {df_clean['content_length'].mean():,.0f} chars\")\n",
        "print(f\"   ‚Ä¢ Avg word count: {df_clean['word_count'].mean():,.0f} words\")\n",
        "\n",
        "print(f\"\\n‚úÖ Quality Assurance:\")\n",
        "print(f\"   ‚úì No duplicate content\")\n",
        "print(f\"   ‚úì All columns preserved\")\n",
        "print(f\"   ‚úì Data integrity maintained\")\n",
        "print(f\"   ‚úì Ready for Soda Core validation\")\n",
        "\n",
        "print(f\"\\nüéØ Next Steps:\")\n",
        "print(f\"   1. ‚úÖ Data is ready at: {output_file}\")\n",
        "print(f\"   2. üìã Use this file for Soda Core setup\")\n",
        "print(f\"   3. üîç Define data quality checks\")\n",
        "print(f\"   4. üìä Monitor data quality metrics\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ EDA & CLEANSING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüí° IMPORTANT NOTES:\")\n",
        "print(f\"   ‚Ä¢ Cleaned data saved to Google Drive and local\")\n",
        "print(f\"   ‚Ä¢ Use 'mbg_data_clean.csv' for Soda Core\")\n",
        "print(f\"   ‚Ä¢ All duplicates removed based on content hash\")\n",
        "print(f\"   ‚Ä¢ Data quality significantly improved\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khygBfBqKID_",
        "outputId": "09922a02-ac65-4cdb-ace2-88180292e3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üîç MBG DATA - EDA & CLEANSING PIPELINE\n",
            "================================================================================\n",
            "Started at: 2025-11-24 15:52:06\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìÇ PART 1: DATA LOADING\n",
            "================================================================================\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "\n",
            "üìÅ Loading data from:\n",
            "   /content/drive/MyDrive/Magister SI TelU/Semester 1/Analisis Data dan Perusahaan/Tugas Besar/Progress Week 9/mbg_data.csv\n",
            "\n",
            "‚úÖ Data loaded successfully!\n",
            "   ‚Ä¢ Shape: 244 rows √ó 10 columns\n",
            "   ‚Ä¢ Memory: 1.08 MB\n",
            "\n",
            "‚úÖ Working copy created for analysis\n",
            "\n",
            "================================================================================\n",
            "üìã PART 2: BASIC INFORMATION\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Column Information:\n",
            "--------------------------------------------------------------------------------\n",
            "    1. url                       | object          |    244/   244 filled (100.0%)\n",
            "    2. title                     | object          |    244/   244 filled (100.0%)\n",
            "    3. date                      | object          |    244/   244 filled (100.0%)\n",
            "    4. author                    | object          |    234/   244 filled ( 95.9%)\n",
            "    5. category                  | object          |    244/   244 filled (100.0%)\n",
            "    6. content                   | object          |    244/   244 filled (100.0%)\n",
            "    7. summary                   | object          |    244/   244 filled (100.0%)\n",
            "    8. tags                      | object          |    244/   244 filled (100.0%)\n",
            "    9. image_url                 | object          |    244/   244 filled (100.0%)\n",
            "   10. scraped_at                | object          |    244/   244 filled (100.0%)\n",
            "\n",
            "2Ô∏è‚É£  Missing Values Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚ö†Ô∏è  author                   :     10 missing ( 4.10%)\n",
            "\n",
            "3Ô∏è‚É£  Data Types Distribution:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ object: 10 columns\n",
            "\n",
            "4Ô∏è‚É£  Sample Data Preview:\n",
            "--------------------------------------------------------------------------------\n",
            "                                                                                                                                    url                                                                                          title                            date              author category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               content                                                                                                                                                                                                                                                                                                                                                       summary                                                                                                                       tags                                                                                       image_url           scraped_at\n",
            "0  https://republika.co.id/berita//t683n9472/waka-bgn-libatkan-peternak-dan-petani-umkm-serta-koperasi-sebagai-pemasok-bahan-pangan-mbg  Waka BGN: Libatkan Peternak dan Petani, UMKM, Serta Koperasi Sebagai Pemasok Bahan Pangan MBG  Senin   24 Nov 2025  15:59 WIB     Ferry kisihandi     News  REPUBLIKA.CO.ID, JOMBANG -- Wakil Kepala Badan Gizi Nasional (BGN) Nanik Sudaryati Deyang memerintahkan kepada Satuan-Satuan Pelayanan Pemenuhan Gizi (SPPG), bersama mitra dan Yayasan untuk melibatkan kelompok peternak ayam kampung dan petani kecil, UMKM, dan koperasi, sebagai pemasok bahan pangan untuk Makan Bergizi Gratis (MBG).\\n\\n‚ÄúSaya ingatkan kepada SPPG-SPPG, Mitra, dan Yayasan, agar jangan hanya memberi kesempatan kepada mereka yang punya modal besar untuk menjadi pemasok bahan pangan MBG. Kalian juga harus melibatkan petani dan peternak kecil, UMKM, dan koperasi,‚Äù kata Nanik dalam Rapat Evaluasi Pelaksanaan Program Makan Bergizi Gratis, di hotel Yusro, Jombang, Jawa Timur, Senin (24/11/2025).\\n\\nDalam kunjungannya ke berbagai daerah, Nanik mengaku mendapatkan pengaduan dari para petani danpeternak kecilserta UMKM.\\n\\nMereka sering mengeluh, hasil pertanian, peternakan, dan usaha mereka tidak bisa masuk ke dapurMBGkarena terkendala soal perizinan, legalitas, dan berbagai aturan usaha lainnya.\\n\\n‚ÄúMereka itu miskin dan nggak punya duit untuk ngurus segala macam. Jadi tolong jangan persulit mereka dengan aturan harus punya NPWP, SIB, UD, dan lain-lain,‚Äù ujarnya.\\n\\nKetua Pelaksana Harian Tim Koordinasi Antar Kementerian dann Lembaga untuk Pelaksanaan Program MBG itu lalu menjelaskan pelibatan petani dan peternak kecil, UMKM, serta koperasi dalam program MBG justru menjadi arahan penting Presiden Prabowo Subianto saat program unggulan pemerintah ini dirancang.\\n\\nSebab, program MBG memang dirancang agar dapat menghidupan perekonomimasyarakat bawahdi berbagai pelosok tanah air.\\n\\n‚ÄúJadi tolong beri kesempatan mereka untuk ikut terlibat sebagai pemasok bahan pangan MBG juga,‚Äù kata Nanik.\\n\\nPelibatan petani, peternak, UMKM, dan koperasi sebagai pemasok bahan pangan secara masif juga sangat bermanfaat untuk menekan angka inflasi.\\n\\nSebab, dengan semakin banyaknya SPPG yang beroperasi, kebutuhan bahan pangan meningkat, dan harga pun naik. Harga bisa ditekan jika pasokan meningkat. ‚ÄúIngat, dalam rapat inflasi awal pekan kemarin, Jombang di peringkat pertama inflasi pangan,‚Äù kata Nanik.\\n\\nKarena itu, Nanik mengimbau seluruh Kepala SPPG, Ahli Gizi, Akuntan, Mitra dan juga Yayasan pengelola MBG memiliki nurani kemanusiaan seperti Presiden Prabowo Subianto, Ketika mengelola dapur-dapur MBG.\\n\\n‚ÄúSaya berharap Anda semua jangan bisnis oriented, saya minta anda juga memiliki nurani kemanusiaan nurani Presiden P{rabowo Subianto,‚Äù ujar mantan wartawan senior itu.                  REPUBLIKA.CO.ID, JOMBANG -- Wakil Kepala Badan Gizi Nasional (BGN) Nanik Sudaryati Deyang memerintahkan kepada Satuan-Satuan Pelayanan Pemenuhan Gizi (SPPG), bersama mitra dan Yayasan untuk melibatkan kelompok peternak ayam kampung dan petani kecil, UMKM, dan koperasi, sebagai pemasok bahan pangan untuk Makan Bergizi Gratis (MBG).                                                           badan gizi nasional, mbg, peternak kecil, umkm, masyarakat bawah  https://static.republika.co.id/uploads/images/inpicture_slide/048168500-1758908462-830-556.jpg  2025-11-24 11:42:17\n",
            "1                                    https://republika.co.id/berita//t668hc423/viral-menu-mewah-mbg-bumil-dan-busui-terima-bakakak-ayam                                      Viral Menu Mewah MBG, Bumil dan Busui Terima Bakakak Ayam   Ahad   23 Nov 2025  15:48 WIB         Gita Amanda     News                                                                                                                                                                                                                      REPUBLIKA.CO.ID,¬†KUNINGAN -- Menu makan bergizi gratis (MBG) yang dibagikan oleh dapur Stasiun Pelayanan Pemenuhan Gizi (SPPG) di Desa Pagundan, Kecamatan Lebakwangi, Kabupaten Kuningan, mendadak viral. Sebab, menu makanan yang dibagikan tergolong mewah, yakni bakakak ayam utuh.\\n\\nDalam video yang beredar di medsos, terlihat deretan paketMBGyang menampilkan makanan bakakak ayam bakar. Ayam yang berukuran cukup besar itu terlihat diberi alas daun pisang dan disertai pula dengan potongan mentimun, kol dan tomat segar. Menu itu juga dilengkapi dengan jeruk.\\n\\nMenu makanan bakakak ayam itu dibagikan kepada ibu hamil dan ibu menyusui yang menjadi penerima program MBG dari SPPG milik Yayasan Baitul Izzah Qur‚Äôani tersebut. Jumlahnya ada 105 penerima.\\n\\nSelain itu, program MBG dari SPPG tersebut juga dibagikan kepada balita sebanyak 361 anak. Namun, untuk Balita¬†bukan ayam bakakak, melainkan susu, biskuit dan telur ayam.\\n\\nMenu MBG yang dibagikan pada Jumat (21/11/2025) itupun disambut antusias oleh para penerimanya. Mereka mengaku baru kali ini menerima paket MBG berupa bakakak ayam.¬†‚ÄúSeneng banget dapat bakakak ayam. Ini spesial karenakankita juga jarang masak bakakak ayam di rumah,‚Äù ujar seorang ibu hamil yang menjadi penerima MBG tersebut, Sri Mulyati.\\n\\nSri mengatakan, menu bakakak ayam itu tidak mungkin dihabiskannya sendirian karena ukurannya yang memang besar. Karena itu, bakakak ayam itu akan dimakannya bersama dengan keluarganya.\\n\\nTak hanya bakakak ayam, anak Sri yang merupakan balita berumur empat tahun juga menjadi penerima program MBG. Karena itu, dalam waktu bersamaan, ia juga menerima paket MBG berupa susu, biskuit anak dan telur ayam untuk buah hatinya.\\n\\n‚ÄúYa dapatdouble(MBG untuk ibu hamil dan MBG untuk balita),‚Äù jelasnya.\\n\\nSementara itu, Kepala SPPG Yayasan Baitul Izzah Qur'ani, Lambang Nuzaman, mengatakan, sengaja menyajikan menu yang bervariatif kepada para penerima program MBG. Ia pun tak menyangka menu bakakak ayam itu mendapat sambutan yang sangat positif dari warga.\\n\\n‚ÄúMakanan inipun sudah dihitung kadar gizinya oleh ahli gizi kita,‚Äù katanya.\\n\\nSoal biaya per porsi makanannya, Lambang menyatakan, sudah sesuai dengan juknis yang telah diberikan oleh negara. Petugas dapur pun tidak kesulitan mengolah ayam bakakak tersebut.                                                                       REPUBLIKA.CO.ID,¬†KUNINGAN -- Menu makan bergizi gratis (MBG) yang dibagikan oleh dapur Stasiun Pelayanan Pemenuhan Gizi (SPPG) di Desa Pagundan, Kecamatan Lebakwangi, Kabupaten Kuningan, mendadak viral. Sebab, menu makanan yang dibagikan tergolong mewah, yakni bakakak ayam utuh.                                                                                            mbg, menu mbg, mbg ayam bekakak  https://static.republika.co.id/uploads/images/inpicture_slide/016631500-1760086660-830-556.jpg  2025-11-24 11:42:20\n",
            "2                              https://republika.co.id/berita//t65tgp484/menko-zulhas-ungkap-program-mbg-pada-2026-butuh-829-juta-porsi                                Menko Zulhas Ungkap Program MBG pada 2026 Butuh 82,9 Juta Porsi   Ahad   23 Nov 2025  10:24 WIB  Erik Purnama Putra     News                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   REPUBLIKA.CO.ID,¬†JAKARTA -- Menteri Koordinator (Menko) bidang Pangan Zulkifli Hasan (Zulhas) mengatakan, program Makan Bergizi Gratis (MBG) pada 2026 membutuhkan 82,9 juta porsi protein, selaras dengan jumlah penerima manfaat.¬†Untuk memenuhi kebutuhan tersebut, Zulhas mengakui, pemerintah sedang bekerja keras dalam mencari sumber-sumber protein.\\n\\n\"Karena tahun depan akan memberikan MBG kepada 82,9 juta penerima manfaat, maka kalau telur satu hari satu, kita perlu 82,9 juta butir telur. Kalau ikan, maka perlu 82,9 juta potong ikan,\" ucap Zulhas dalam acara PuncakHari Ikan Nasional 2025di kawasan Sarinah, Jakarta Pusat, Ahad (23/11/2025).\\n\\nMenurt dia, apabila tidak terdapat penambahan sumber protein, sementara kebutuhan protein meningkat, terdapat kemungkinan harganya meroket di pasar.¬†\"Kan biasa hukum pasar, kalau yang minta banyak, permintaan banyak, tetapi produksi sedikit, pasti harganya naik. Makanya kami sekarang sedang berlomba-lomba,\" kata Zulhas.\\n\\nOleh karena itu, sambung dia, pemerintah mempertimbangkan untuk menggunakan sumber protein khas masing-masing daerah. Zulhas menyoroti, perbedaan makanan khas wilayah Sumatra dengan Papua.\\n\\nPerbedaan tersebut, menurut dia, bisa menjadi salah satu solusi untuk memenuhi kebutuhan protein program MBG. \"Di Sumatra mungkin sukanya ikan, di Papua beda makanannya, di Jawa mungkin sukanya berbeda. Ini sudah kami tata, kami perlihatkan nanti begitu beragamnya makanan Indonesia,\"¬†ucap Zulhas.\\n\\nDalam kesempatan tersebut, Zulhas juga mengumumkan target Indonesia mencapai swasembada protein pada 2026 ketika memperingati Hari Ikan Nasional. Menurut dia, protein berperan penting untuk meningkatkan kecerdasan bangsa, khususnya generasi muda yang saat ini masih menempuh jenjang pendidikan.  REPUBLIKA.CO.ID,¬†JAKARTA -- Menteri Koordinator (Menko) bidang Pangan Zulkifli Hasan (Zulhas) mengatakan, program Makan Bergizi Gratis (MBG) pada 2026 membutuhkan 82,9 juta porsi protein, selaras dengan jumlah penerima manfaat.¬†Untuk memenuhi kebutuhan tersebut, Zulhas mengakui, pemerintah sedang bekerja keras dalam mencari sumber-sumber protein.  program mbg, menko zulhas, menko zulkifli hasan, makan bergizi gratis, program mbg butuh protein, hari ikan nasional 2025  https://static.republika.co.id/uploads/images/inpicture_slide/017765400-1763864876-830-556.jpg  2025-11-24 11:42:23\n",
            "\n",
            "================================================================================\n",
            "üîç PART 3: DETAILED DUPLICATE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Duplicate Analysis by Different Criteria:\n",
            "--------------------------------------------------------------------------------\n",
            "   üìå URL Analysis:\n",
            "      ‚Ä¢ Total unique URLs: 244\n",
            "      ‚Ä¢ Duplicate URL entries: 0\n",
            "      ‚Ä¢ URL uniqueness: 100.0%\n",
            "\n",
            "   üìå Title Analysis:\n",
            "      ‚Ä¢ Total unique titles: 244\n",
            "      ‚Ä¢ Duplicate title entries: 0\n",
            "      ‚Ä¢ Title uniqueness: 100.0%\n",
            "\n",
            "   üìå Content Analysis:\n",
            "      ‚Ä¢ Total unique contents: 244\n",
            "      ‚Ä¢ Duplicate content entries: 0\n",
            "      ‚Ä¢ Content uniqueness: 100.0%\n",
            "\n",
            "2Ô∏è‚É£  True Duplicates (Same Content):\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚úÖ No duplicate content found!\n",
            "\n",
            "3Ô∏è‚É£  Duplicate Detection Conclusion:\n",
            "--------------------------------------------------------------------------------\n",
            "   üìä Summary:\n",
            "      ‚Ä¢ Original dataset: 244 rows\n",
            "      ‚Ä¢ Unique by URL: 244 rows (100.0%)\n",
            "      ‚Ä¢ Unique by Title: 244 rows (100.0%)\n",
            "      ‚Ä¢ Unique by Content: 244 rows (100.0%)\n",
            "\n",
            "   üí° Recommendation:\n",
            "      ‚Ä¢ Use CONTENT HASH for deduplication\n",
            "      ‚Ä¢ Expected clean data: ~244 unique articles\n",
            "\n",
            "================================================================================\n",
            "üìä PART 4: CONTENT STATISTICS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Content Length Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Mean:     2,211 characters\n",
            "   ‚Ä¢ Median:   2,092 characters\n",
            "   ‚Ä¢ Min:      296 characters\n",
            "   ‚Ä¢ Max:      6,378 characters\n",
            "   ‚Ä¢ Std Dev:  907 characters\n",
            "\n",
            "2Ô∏è‚É£  Word Count Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Mean:     298 words\n",
            "   ‚Ä¢ Median:   279 words\n",
            "   ‚Ä¢ Min:      42 words\n",
            "   ‚Ä¢ Max:      837 words\n",
            "\n",
            "3Ô∏è‚É£  Content Length Distribution:\n",
            "--------------------------------------------------------------------------------\n",
            "   <500      :    10 articles (  4.1%) ‚ñà‚ñà\n",
            "   500-1K    :     2 articles (  0.8%) \n",
            "   1K-2K     :    96 articles ( 39.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   2K-3K     :   103 articles ( 42.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   3K-5K     :    30 articles ( 12.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   >5K       :     3 articles (  1.2%) \n",
            "\n",
            "================================================================================\n",
            "üè∑Ô∏è  PART 5: CATEGORY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Category Distribution:\n",
            "--------------------------------------------------------------------------------\n",
            "    1. News                          :   101 ( 41.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    2. Rejabar                       :    61 ( 25.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    3. Rejogja                       :    25 ( 10.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    4. Ekonomi                       :    23 (  9.4%) ‚ñà‚ñà‚ñà‚ñà\n",
            "    5. Visual                        :    10 (  4.1%) ‚ñà‚ñà\n",
            "    6. Ameera                        :     9 (  3.7%) ‚ñà\n",
            "    7. Islam Digest                  :     9 (  3.7%) ‚ñà\n",
            "    8. Ekonomi Syariah               :     2 (  0.8%) \n",
            "    9. Kolom                         :     2 (  0.8%) \n",
            "   10. En                            :     1 (  0.4%) \n",
            "   11. Esgnow                        :     1 (  0.4%) \n",
            "\n",
            "2Ô∏è‚É£  Category Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Total unique categories: 11\n",
            "   ‚Ä¢ Most common: News (101 articles)\n",
            "   ‚Ä¢ Least common: Esgnow (1 articles)\n",
            "   ‚Ä¢ Articles without category: 0\n",
            "\n",
            "================================================================================\n",
            "‚úçÔ∏è  PART 6: AUTHOR ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Top 15 Most Productive Authors:\n",
            "--------------------------------------------------------------------------------\n",
            "    1. Arie Lukihardianti                      :   47 articles ( 19.3%)\n",
            "    2. Erik Purnama Putra                      :   22 articles (  9.0%)\n",
            "    3. Mas Alamil Huda                         :   20 articles (  8.2%)\n",
            "    4. Andri Saubani                           :   20 articles (  8.2%)\n",
            "    5. Karta Raharja Ucu                       :   19 articles (  7.8%)\n",
            "    6. Teguh Firmansyah                        :   16 articles (  6.6%)\n",
            "    7. Fernan Rahadi                           :   11 articles (  4.5%)\n",
            "    8. Friska Yolandha                         :   10 articles (  4.1%)\n",
            "    9. Unknown                                 :   10 articles (  4.1%)\n",
            "   10. Hasanul Rizqa                           :    8 articles (  3.3%)\n",
            "   11. Qommarria Rostanti                      :    8 articles (  3.3%)\n",
            "   12. Ferry kisihandi                         :    8 articles (  3.3%)\n",
            "   13. Gita Amanda                             :    7 articles (  2.9%)\n",
            "   14. Satria K Yudha                          :    7 articles (  2.9%)\n",
            "   15. Muhammad Hafil                          :    5 articles (  2.0%)\n",
            "\n",
            "2Ô∏è‚É£  Author Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Total unique authors: 28\n",
            "   ‚Ä¢ Articles without author: 10\n",
            "   ‚Ä¢ Average articles per author: 8.7\n",
            "\n",
            "================================================================================\n",
            "üìÖ PART 7: DATE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Date Format Examples:\n",
            "--------------------------------------------------------------------------------\n",
            "   1. Senin   24 Nov 2025  15:59 WIB\n",
            "   2. Ahad   23 Nov 2025  15:48 WIB\n",
            "   3. Ahad   23 Nov 2025  10:24 WIB\n",
            "   4. Ahad   23 Nov 2025  05:36 WIB\n",
            "   5. Sabtu   22 Nov 2025  15:05 WIB\n",
            "\n",
            "2Ô∏è‚É£  Date Completeness:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Articles with date: 244 (100.0%)\n",
            "   ‚Ä¢ Articles without date: 0 (0.0%)\n",
            "\n",
            "3Ô∏è‚É£  Articles by Year:\n",
            "--------------------------------------------------------------------------------\n",
            "   2025:   244 articles (100.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "================================================================================\n",
            "üçΩÔ∏è  PART 8: MBG-SPECIFIC CONTENT ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  Distribution Content Detection:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Articles about distribution: 86/244 (35.2%)\n",
            "\n",
            "2Ô∏è‚É£  Quantitative Information:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Articles with portion info: 13\n",
            "   ‚Ä¢ Articles with recipient info: 85\n",
            "   ‚Ä¢ Articles with complete info: 5\n",
            "\n",
            "================================================================================\n",
            "üßπ PART 9: DATA CLEANSING\n",
            "================================================================================\n",
            "\n",
            "üìä Before Cleaning:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Total rows: 244\n",
            "   ‚Ä¢ Unique content: 244\n",
            "   ‚Ä¢ Duplicates to remove: 0\n",
            "\n",
            "1Ô∏è‚É£  Duplicate Removal:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚úÖ Removed 0 duplicate articles\n",
            "   ‚úÖ Retained 244 unique articles\n",
            "\n",
            "2Ô∏è‚É£  Column Cleanup:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚úÖ Removed temporary analysis columns\n",
            "   ‚úÖ Final columns: 13\n",
            "\n",
            "3Ô∏è‚É£  Missing Values Handling:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ author: 10 missing values (kept as is)\n",
            "\n",
            "üìä After Cleaning:\n",
            "--------------------------------------------------------------------------------\n",
            "   ‚Ä¢ Total rows: 244\n",
            "   ‚Ä¢ Total columns: 13\n",
            "   ‚Ä¢ Memory usage: 1.08 MB\n",
            "   ‚Ä¢ Reduction: 0.0%\n",
            "\n",
            "================================================================================\n",
            "üíæ PART 10: SAVING CLEANED DATA\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Data saved to Google Drive:\n",
            "   üìÅ /content/drive/MyDrive/Magister SI TelU/Semester 1/Analisis Data dan Perusahaan/Tugas Besar/Progress Week 9/mbg_data_clean.csv\n",
            "   üìä Size: 0.71 MB\n",
            "\n",
            "‚úÖ Data also saved locally:\n",
            "   üìÅ /content/mbg_data_clean.csv\n",
            "\n",
            "================================================================================\n",
            "üìã PART 11: FINAL SUMMARY REPORT\n",
            "================================================================================\n",
            "\n",
            "üéØ EDA & CLEANSING SUMMARY:\n",
            "================================================================================\n",
            "\n",
            "üìä Dataset Transformation:\n",
            "   ‚Ä¢ Original dataset: 244 articles\n",
            "   ‚Ä¢ Cleaned dataset: 244 articles\n",
            "   ‚Ä¢ Removed duplicates: 0 articles\n",
            "   ‚Ä¢ Data reduction: 0.0%\n",
            "\n",
            "üîç Data Quality Metrics:\n",
            "   ‚Ä¢ Content uniqueness: 100.0%\n",
            "   ‚Ä¢ Date completeness: 100.0%\n",
            "   ‚Ä¢ Author completeness: 95.9%\n",
            "   ‚Ä¢ Category completeness: 100.0%\n",
            "\n",
            "üìà Content Characteristics:\n",
            "   ‚Ä¢ Unique categories: 11\n",
            "   ‚Ä¢ Unique authors: 28\n",
            "   ‚Ä¢ Avg content length: 2,211 chars\n",
            "   ‚Ä¢ Avg word count: 298 words\n",
            "\n",
            "‚úÖ Quality Assurance:\n",
            "   ‚úì No duplicate content\n",
            "   ‚úì All columns preserved\n",
            "   ‚úì Data integrity maintained\n",
            "   ‚úì Ready for Soda Core validation\n",
            "\n",
            "üéØ Next Steps:\n",
            "   1. ‚úÖ Data is ready at: mbg_data_clean.csv\n",
            "   2. üìã Use this file for Soda Core setup\n",
            "   3. üîç Define data quality checks\n",
            "   4. üìä Monitor data quality metrics\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EDA & CLEANSING COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "Finished at: 2025-11-24 15:52:09\n",
            "================================================================================\n",
            "\n",
            "üí° IMPORTANT NOTES:\n",
            "   ‚Ä¢ Cleaned data saved to Google Drive and local\n",
            "   ‚Ä¢ Use 'mbg_data_clean.csv' for Soda Core\n",
            "   ‚Ä¢ All duplicates removed based on content hash\n",
            "   ‚Ä¢ Data quality significantly improved\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_Y4NUQXK9kS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}